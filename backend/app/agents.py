from typing import Dict, Any, List
import httpx
import json

OLLAMA_BASE_URL = "http://localhost:11434"
OLLAMA_GENERATOR_MODEL = "codellama:34b"  # For code generation and formal definitions
OLLAMA_EXPLAINER_MODEL = "deepseek-coder:33b"  # For explanations and educational reasoning
OLLAMA_VISION_MODEL = "llava:34b"  # For image processing and OCR

class AutomataGenerator:
    """Uses codellama:34b for formal automata generation and code"""
    
    async def generate_automaton(self, task: str, problem_type: str = "dfa") -> Dict[str, Any]:
        """Generate formal automaton structure and implementation code"""
        prompt = f"""
You are an expert in Theory of Computation and formal automata. Generate a complete {problem_type.upper()} for this task:

Task: {task}

Please provide a structured response with:

1. FORMAL DEFINITION:
   - States (Q)
   - Alphabet (Σ)
   - Transition function (δ)
   - Start state (q₀)
   - Accept states (F)

2. PYTHON IMPLEMENTATION:
   - Complete simulation function
   - State transition logic
   - String acceptance testing

3. DOT GRAPH CODE:
   - Graphviz DOT notation for visualization
   - Proper state and transition labeling

4. TEST CASES:
   - 5 strings that should be accepted
   - 5 strings that should be rejected

Format your response as JSON with keys: formal_definition, python_code, dot_graph, test_cases
"""

        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{OLLAMA_BASE_URL}/api/generate",
                    json={
                        "model": OLLAMA_GENERATOR_MODEL,
                        "prompt": prompt,
                        "stream": False
                    },
                    timeout=25.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    ai_response = result.get("response", "")
                    
                    try:
                        return json.loads(ai_response)
                    except json.JSONDecodeError:
                        return {
                            "formal_definition": "Generated by AI",
                            "python_code": ai_response,
                            "dot_graph": "// Generated DOT code",
                            "test_cases": {"accept": [], "reject": []}
                        }
                else:
                    return self._fallback_response(task, problem_type)
                    
        except Exception as e:
            return self._fallback_response(task, problem_type)
    
    def _fallback_response(self, task: str, problem_type: str) -> Dict[str, Any]:
        """Fallback response when AI is unavailable"""
        return {
            "formal_definition": f"Please manually design a {problem_type.upper()} for: {task}",
            "python_code": "# AI code generation temporarily unavailable",
            "dot_graph": "// DOT graph generation temporarily unavailable",
            "test_cases": {"accept": ["example1"], "reject": ["example2"]}
        }
    
    async def is_toc_problem(self, problem_text: str, problem_type: str = "text") -> bool:
        """Determine if the input is a Theory of Computation problem"""
        if problem_type == "image":
            return True
            
        prompt = f"""
        Analyze this text to determine if it's a Theory of Computation problem:
        
        Text: {problem_text}
        
        Return only "YES" if this is related to:
        - Automata (DFA, NFA, PDA, Turing Machines)
        - Formal languages and grammars
        - Regular expressions
        - Computability theory
        - Pumping lemma
        - Language equivalence or minimization
        
        Return "NO" if it's about other computer science topics or not CS-related.
        """
        
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{OLLAMA_BASE_URL}/api/generate",
                    json={
                        "model": OLLAMA_GENERATOR_MODEL,
                        "prompt": prompt,
                        "stream": False
                    },
                    timeout=10.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    ai_response = result.get("response", "").strip().upper()
                    return "YES" in ai_response
                else:
                    return True
                    
        except Exception as e:
            return True
    
    async def analyze_problem_text(self, problem_text: str, problem_type: str = "text") -> Dict[str, Any]:
        """Analyze problem text and generate complete solution"""
        if problem_type == "image":
            return await self._process_image_problem(problem_text)
        
        prompt = f"""
        Analyze this Theory of Computation problem and provide a complete analysis:
        
        Problem: {problem_text}
        
        Please provide a JSON response with:
        {{
            "automaton_type": "dfa|nfa|pda|cfg|tm|regex|pumping",
            "description": "Clear problem statement",
            "difficulty": "beginner|intermediate|advanced",
            "concepts": ["list", "of", "key", "concepts"],
            "solution": {{
                "formal_definition": "5-tuple or formal definition",
                "python_code": "Complete implementation",
                "dot_graph": "Graphviz DOT code",
                "explanation": "Step-by-step explanation"
            }},
            "guided_steps": [
                "Step 1: Understand the language",
                "Step 2: Identify states needed",
                "Step 3: Define transitions",
                "Step 4: Mark accept states",
                "Step 5: Test with examples"
            ],
            "test_cases": {{
                "accept": ["string1", "string2"],
                "reject": ["string3", "string4"]
            }}
        }}
        """
        
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{OLLAMA_BASE_URL}/api/generate",
                    json={
                        "model": OLLAMA_GENERATOR_MODEL,
                        "prompt": prompt,
                        "stream": False
                    },
                    timeout=25.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    ai_response = result.get("response", "")
                    
                    try:
                        return json.loads(ai_response)
                    except json.JSONDecodeError:
                        return self._fallback_problem_analysis(problem_text)
                else:
                    return self._fallback_problem_analysis(problem_text)
                    
        except Exception as e:
            return self._fallback_problem_analysis(problem_text)
    
    async def _process_image_problem(self, base64_image: str) -> Dict[str, Any]:
        """Process image using llava:34b to extract TOC problem"""
        prompt = """
        Analyze this image and extract any Theory of Computation problem you can find.
        
        Look for:
        - Problem statements about automata (DFA, NFA, PDA, TM)
        - Language definitions
        - Regular expressions
        - Grammar rules
        - Pumping lemma problems
        
        If you find a TOC problem, provide a JSON response with:
        {
            "automaton_type": "dfa|nfa|pda|cfg|tm|regex|pumping",
            "description": "Extracted problem statement",
            "difficulty": "beginner|intermediate|advanced",
            "concepts": ["list", "of", "key", "concepts"],
            "solution": {
                "formal_definition": "5-tuple or formal definition",
                "python_code": "Complete implementation",
                "dot_graph": "Graphviz DOT code",
                "explanation": "Step-by-step explanation"
            },
            "guided_steps": [
                "Step 1: Understand the language",
                "Step 2: Identify states needed",
                "Step 3: Define transitions",
                "Step 4: Mark accept states",
                "Step 5: Test with examples"
            ],
            "test_cases": {
                "accept": ["string1", "string2"],
                "reject": ["string3", "string4"]
            }
        }
        
        If no TOC problem is found, return: {"error": "No Theory of Computation problem found in image"}
        """
        
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{OLLAMA_BASE_URL}/api/generate",
                    json={
                        "model": OLLAMA_VISION_MODEL,
                        "prompt": prompt,
                        "images": [base64_image.split(',')[1] if ',' in base64_image else base64_image],
                        "stream": False
                    },
                    timeout=30.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    ai_response = result.get("response", "")
                    
                    try:
                        parsed_result = json.loads(ai_response)
                        if "error" in parsed_result:
                            return self._fallback_problem_analysis("Image analysis failed")
                        return parsed_result
                    except json.JSONDecodeError:
                        return self._fallback_problem_analysis("Image contains TOC problem but parsing failed")
                else:
                    return self._fallback_problem_analysis("Image processing failed")
                    
        except Exception as e:
            return self._fallback_problem_analysis(f"Image processing error: {str(e)}")
    
    def _fallback_problem_analysis(self, problem_text: str) -> Dict[str, Any]:
        """Fallback problem analysis when AI is unavailable"""
        return {
            "automaton_type": "dfa",
            "description": problem_text,
            "difficulty": "intermediate",
            "concepts": ["automata", "formal-languages"],
            "solution": self._fallback_response(problem_text, "dfa"),
            "guided_steps": [
                "Step 1: Identify the language to be recognized",
                "Step 2: Determine what information needs to be tracked",
                "Step 3: Design states to represent this information",
                "Step 4: Define transitions between states",
                "Step 5: Mark appropriate accept states"
            ],
            "test_cases": {"accept": ["example"], "reject": ["counter-example"]}
        }

    async def generate_proof_steps(self, automaton_data: Dict[str, Any], proof_type: str, current_steps: List[Dict]) -> Dict[str, Any]:
        """Generate proof step suggestions"""
        prompt = f"""
        Generate proof steps for {proof_type} proof:
        
        Automaton: {automaton_data}
        Current steps: {current_steps}
        
        Please suggest:
        1. Next logical proof steps
        2. Mathematical reasoning
        3. Key lemmas or theorems to apply
        4. Step-by-step approach
        """
        
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{OLLAMA_BASE_URL}/api/generate",
                    json={
                        "model": OLLAMA_GENERATOR_MODEL,
                        "prompt": prompt,
                        "stream": False
                    },
                    timeout=25.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result.get("response", "")
                    return {
                        "steps": [
                            {
                                "description": "Apply the definition of regular languages",
                                "type": "definition",
                                "explanation": "Start with formal definitions"
                            },
                            {
                                "description": "Construct the cross-product automaton",
                                "type": "lemma", 
                                "explanation": "Use standard construction techniques"
                            }
                        ],
                        "reasoning": content,
                        "next_steps": ["Verify construction", "Check acceptance conditions"]
                    }
                else:
                    return {
                        "steps": [],
                        "reasoning": "AI proof generation temporarily unavailable",
                        "next_steps": ["Please try a different approach"]
                    }
        except Exception as e:
            return {
                "steps": [],
                "reasoning": f"Error generating proof: {str(e)}",
                "next_steps": ["Please try a different approach"]
            }

class AutomataExplainer:
    """Uses deepseek-coder:33b for educational explanations and step-by-step reasoning"""
    
    async def explain_automaton(self, task: str, automaton_data: Dict[str, Any], user_automaton: Any = None) -> Dict[str, Any]:
        """Provide detailed educational explanation of the automaton"""
        prompt = f"""
You are an expert Theory of Computation tutor. A student is learning about automata and needs a clear, educational explanation.

Task: {task}

Generated Automaton:
{json.dumps(automaton_data, indent=2)}

Please provide a comprehensive educational explanation including:

1. CONCEPTUAL OVERVIEW:
   - What this automaton does in simple terms
   - Why this design works for the given language

2. STATE-BY-STATE EXPLANATION:
   - Purpose of each state
   - What each state "remembers" or tracks

3. TRANSITION LOGIC:
   - Why each transition exists
   - How symbols affect state changes

4. EXAMPLE WALKTHROUGH:
   - Step-by-step execution on an accepting string
   - Step-by-step execution on a rejecting string

5. COMMON MISTAKES:
   - What students often get wrong
   - How to avoid these pitfalls

6. LEARNING INSIGHTS:
   - Key automata theory concepts demonstrated
   - Connections to other CS concepts

Be encouraging, clear, and pedagogically sound. Use analogies where helpful.
"""

        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{OLLAMA_BASE_URL}/api/generate",
                    json={
                        "model": OLLAMA_EXPLAINER_MODEL,
                        "prompt": prompt,
                        "stream": False
                    },
                    timeout=15.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    explanation = result.get("response", "")
                    
                    return {
                        "explanation": explanation,
                        "key_concepts": self._extract_concepts(explanation),
                        "next_steps": self._suggest_next_steps(task)
                    }
                else:
                    return self._fallback_explanation(task)
                    
        except Exception as e:
            return self._fallback_explanation(task)
    
    async def provide_step_guidance(self, task: str, current_progress: Dict[str, Any]) -> str:
        """Provide real-time step-by-step guidance as student builds automaton"""
        prompt = f"""
You are a Theory of Computation professor providing real-time guidance to a student.

Task: {task}

Student's Current Progress:
- States: {current_progress.get('states', 0)}
- Transitions: {current_progress.get('transitions', 0)}
- Start States: {current_progress.get('start_states', 0)}
- Accept States: {current_progress.get('accept_states', 0)}
- Current States: {current_progress.get('state_list', [])}
- Current Transitions: {current_progress.get('transition_list', [])}

Provide ONE specific, actionable next step. Be very specific about:
- Exactly which state to add next (if needed) and where to place it
- Which transitions to add and between which specific states
- Which states should be marked as start/accept states
- Be encouraging and educational but focus on the immediate next action.

Format: "Next step: [specific action]"
"""

        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{OLLAMA_BASE_URL}/api/generate",
                    json={
                        "model": OLLAMA_EXPLAINER_MODEL,
                        "prompt": prompt,
                        "stream": False
                    },
                    timeout=10.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result.get("response", "Consider what states you need to track the problem requirements.")
                else:
                    return "Consider what states you need to track the problem requirements."
                    
        except Exception as e:
            return "Consider what states you need to track the problem requirements."
    
    def _extract_concepts(self, explanation: str) -> List[str]:
        """Extract key learning concepts from explanation"""
        concepts = []
        lines = explanation.lower().split('\n')
        
        concept_keywords = ['state', 'transition', 'language', 'alphabet', 'deterministic', 'nondeterministic']
        for line in lines:
            for keyword in concept_keywords:
                if keyword in line and len(line.strip()) < 100:
                    concepts.append(line.strip())
                    break
        
        return concepts[:5]  # Return top 5 concepts
    
    def _suggest_next_steps(self, task: str) -> List[str]:
        """Suggest next learning steps"""
        return [
            "Try building this automaton step by step",
            "Test your automaton with the provided examples",
            "Consider variations of this problem",
            "Explore related automata theory concepts"
        ]
    
    def _fallback_explanation(self, task: str) -> Dict[str, Any]:
        """Fallback explanation when AI is unavailable"""
        return {
            "explanation": f"This automaton solves the problem: {task}. Consider the states needed to track progress toward accepting valid strings.",
            "key_concepts": ["states", "transitions", "acceptance"],
            "next_steps": ["Build the automaton step by step", "Test with examples"]
        }

    async def validate_proof_step(self, automaton_data: Dict[str, Any], proof_type: str, step_id: str, steps: List[Dict]) -> Dict[str, Any]:
        """Validate a proof step using AI reasoning"""
        prompt = f"""
        Validate this proof step for {proof_type} proof:
        
        Automaton: {automaton_data}
        Current steps: {steps}
        Step to validate: {step_id}
        
        Please analyze:
        1. Is this step logically valid?
        2. Does it follow from previous steps?
        3. What are potential issues?
        4. Suggestions for improvement
        """
        
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{OLLAMA_BASE_URL}/api/generate",
                    json={
                        "model": "acereason-nemotron-abliterated",
                        "prompt": prompt,
                        "stream": False
                    },
                    timeout=15.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result.get("response", "")
                    return {
                        "is_valid": "valid" in content.lower() and "invalid" not in content.lower(),
                        "explanation": content,
                        "suggestions": ["Review logical flow", "Check mathematical rigor"]
                    }
                else:
                    return {
                        "is_valid": False,
                        "explanation": "AI validation temporarily unavailable",
                        "suggestions": ["Please try again"]
                    }
        except Exception as e:
            return {
                "is_valid": False,
                "explanation": f"Error validating step: {str(e)}",
                "suggestions": ["Please try again"]
            }
